{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn Introductory Tutorial\n",
    "\n",
    "[scikit-learning](http://scikit-learn.org/stable/index.html) is an open-sourced simple and efficient tools for data mining, data analysis and machine learning in Python. It is built on NumPy, SciPy and matplotlib. There are built-in classification, regression, and clustering models, as well as useful features like dimensionality reduction, evaluation and preprocessing. \n",
    "\n",
    "This tutorial is specifically tailored for NLP, i.e. working with text data. It will cover the following topics: loading data, preprocessing, feature extraction, training, evaluation, grid search, building a pipeline, creating custom transformers, etc.\n",
    "\n",
    "For this tutorial, we will use the [20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/) and perform topic classification. For the sake of time, I converted all the data into a CSV file. \n",
    "\n",
    "Note: apparently Jupyter disables spell-checker, so I'm only partially responsible for the typos in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset\n",
    "\n",
    "For the sake of convenience, we will use pandas to read CSV file. (You may do so with numpy as well; there is a `loadtext()` function, but you might encounter encoding issues when using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('20news-18828.csv', header=None, delimiter=',', names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 categories: True\n",
      "There are 18828 records: True\n"
     ]
    }
   ],
   "source": [
    "print(\"There are 20 categories: %s\" % (len(dataset.label.unique()) == 20))\n",
    "print(\"There are 18828 records: %s\" % (len(dataset) == 18828))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split it to train set and test set. To do so, we can use the `train_test_split()` function. In scikit-learn's convention, X indicates data (yeah, uppercase X), and y indicates truths (and yeah, lowercase y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.text, dataset.label, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A Simple Example\n",
    "\n",
    "Before going too much into preprocessing, feature extraction and other more complicated tasks, we will do a relatively simple but complete example. In this example, we will use bag-of-words as features, and Naive Bayes as classifier to establish our baseline.\n",
    "\n",
    "There are some built-in vectorizers, `CountVectorizer` and `TfidfVectorizer` that we can use to vectorizer our raw data and perform preprocessing and feature exctration on it. First, we will experiment with `CountVectorizer` which basically makes a token/ngram a feature and stores its count in the corresponding feature space. The `fit_transform()` function is the combination of `fit()` and `transform()`. `fit()` learns the vocabulary/features of a document, and `transform()` transforms the dataset into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15062, 181473)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize a CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# fit the raw data into the vectorizer and tranform it into a series of arrays\n",
    "X_train_counts = cv.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar thing needs to be done for the test set, but we only need to use the `transform()` function to transform the test data into a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3766, 181473)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts = cv.transform(X_test)\n",
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit our features and labels into a Naive Bayes classifier, which basically trains a model (if you fit the data more than once, it overwrites the parameters the model learns previously). After training, we can use it to perform prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misc.forsale misc.forsale\n",
      "talk.politics.guns talk.politics.guns\n",
      "comp.sys.mac.hardware comp.sys.mac.hardware\n",
      "talk.politics.guns talk.politics.guns\n",
      "rec.motorcycles rec.motorcycles\n",
      "talk.politics.mideast talk.politics.mideast\n",
      "rec.motorcycles rec.motorcycles\n",
      "comp.sys.ibm.pc.hardware comp.sys.ibm.pc.hardware\n",
      "soc.religion.christian soc.religion.christian\n",
      "comp.sys.ibm.pc.hardware comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "predicted = clf.predict(X_test_counts)\n",
    "\n",
    "# sample some of the predictions against the ground truths \n",
    "for prediction, truth in zip(predicted[:10], y_test[:10]):\n",
    "    print(prediction, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some legit evaluation. The `classification_report()` function gives you precison, recall and f1 scores for each label, and their average. If you want to calculate overall macro-averaged, micro-averaged or weighted performance, you can use the `precision_recall_fscore_support`. Finally, the `confusion_matrix()` can show you which labels are confusing to the model, but unfortunately, it does not include the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.84      0.86      0.85       152\n",
      "           comp.graphics       0.58      0.88      0.70       188\n",
      " comp.os.ms-windows.misc       0.95      0.18      0.30       205\n",
      "comp.sys.ibm.pc.hardware       0.63      0.87      0.73       189\n",
      "   comp.sys.mac.hardware       0.93      0.84      0.88       199\n",
      "          comp.windows.x       0.79      0.83      0.81       202\n",
      "            misc.forsale       0.92      0.61      0.74       193\n",
      "               rec.autos       0.92      0.92      0.92       194\n",
      "         rec.motorcycles       0.97      0.94      0.96       180\n",
      "      rec.sport.baseball       0.98      0.93      0.96       209\n",
      "        rec.sport.hockey       0.96      0.99      0.98       186\n",
      "               sci.crypt       0.81      0.97      0.88       193\n",
      "         sci.electronics       0.88      0.80      0.84       208\n",
      "                 sci.med       0.93      0.92      0.93       202\n",
      "               sci.space       0.92      0.96      0.94       206\n",
      "  soc.religion.christian       0.73      0.96      0.83       211\n",
      "      talk.politics.guns       0.86      0.94      0.90       174\n",
      "   talk.politics.mideast       0.87      0.99      0.93       182\n",
      "      talk.politics.misc       0.80      0.84      0.82       164\n",
      "      talk.religion.misc       0.92      0.43      0.58       129\n",
      "\n",
      "             avg / total       0.86      0.84      0.83      3766\n",
      "\n",
      "Micro-averaged Performance:\n",
      "Precision: 0.8385554965480616, Recall: 0.8385554965480616, F1: 0.8385554965480616\n",
      "[[131   0   0   0   0   0   0   0   0   0   0   0   0   0   0  13   0   4\n",
      "    2   2]\n",
      " [  0 166   0   3   0   2   0   0   0   1   0   3   2   2   2   4   0   1\n",
      "    2   0]\n",
      " [  0  52  37  56   3  37   1   0   0   0   1   9   1   0   1   2   1   0\n",
      "    4   0]\n",
      " [  0   7   0 165   2   3   2   1   0   0   0   4   2   0   1   1   1   0\n",
      "    0   0]\n",
      " [  0   7   0   9 167   2   0   0   1   0   0   5   4   2   0   1   0   0\n",
      "    1   0]\n",
      " [  0  24   1   1   1 168   1   0   0   0   0   2   0   1   0   2   0   1\n",
      "    0   0]\n",
      " [  1   5   1  19   6   0 118   8   1   1   2   7  12   3   1   1   2   1\n",
      "    4   0]\n",
      " [  0   1   0   0   0   0   4 178   2   0   0   0   0   1   0   1   2   0\n",
      "    5   0]\n",
      " [  0   0   0   0   0   0   1   4 170   0   0   2   0   0   0   0   1   2\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   2   1 195   4   0   0   0   0   3   0   3\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 185   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0 187   0   0   0   0   1   0\n",
      "    1   2]\n",
      " [  0   9   0   6   1   0   1   1   0   0   0  10 167   1   9   2   0   0\n",
      "    1   0]\n",
      " [  0   4   0   1   0   0   0   0   0   0   0   1   0 186   2   8   0   0\n",
      "    0   0]\n",
      " [  0   5   0   0   0   0   0   0   0   0   0   1   0   2 197   0   0   0\n",
      "    1   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   1   1   0 203   0   3\n",
      "    1   1]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   1 164   3\n",
      "    5   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 181\n",
      "    0   0]\n",
      " [  0   1   0   1   0   0   0   0   0   1   0   1   0   0   1   3  12   6\n",
      "  138   0]\n",
      " [ 24   0   0   0   0   0   0   0   0   1   0   0   0   1   1  32   6   4\n",
      "    5  55]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, labels=dataset.label.unique()))\n",
    "\n",
    "p, r, f1, _ = metrics.precision_recall_fscore_support(y_test, predicted, labels=dataset.label.unique(), average='micro')\n",
    "\n",
    "print(\"Micro-averaged Performance:\\nPrecision: {0}, Recall: {1}, F1: {2}\".format(p, r, f1))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, predicted, labels=dataset.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing & Feature Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One may ask, \"how do I remove stop words, tokenize the texts differently, or use bigrams/trigrams as features?\" \n",
    "The answer is you can do all that when you initialize a [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) object, i.e. you can pass various arguments to the constructor. \n",
    "\n",
    "Here are some of them: `ngram_range` takes in a tuple (n_min, n_max). For example, `(2,2)` means only use bigrams, and `(1,3)` means use unigrams, bigrams, and trigrams. `stop_words` takes in a list of stopwords that you'd like to remove. If you want to use default stopword list in scikit-learn, pass in the string `'english'`. `tokenizer` is a function that takes in a string and returns a string, inside that function, you can define how to tokenize your text. By default, scikit-learn tokenization pattern is `u'(?u)\\b\\w\\w+\\b'`. Finally, `preprocessor` takes in a function of which the argument is a string and the output is a string. You can use it to perform more customized preprocessing. For more detail, please checkout the documentation for `CountVectorizer` or `TfidfVectorizer`.\n",
    "\n",
    "Let's start with defining a preprocessor to normalize all the numeric values, i.e. replacing numbers with the string `NUM`. Then, we construct a new `CountVectorizer`, and then use unigrams, bigrams, and trigrams as features, and remove stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_numbers(s):\n",
    "    return re.sub(r'\\b\\d+\\b', 'NUM', s)\n",
    "\n",
    "cv = CountVectorizer(preprocessor=normalize_numbers, ngram_range=(1,3), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit and transform the train data and transform the test data. The speed of preprocessing and feature extraction depends on the running time of each step. For example, the running time of stopword removal is O(N * M), where N is the vocabulary size of the document, and M is the stopword list size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15062, 3615034)\n",
      "(3766, 3615034)\n"
     ]
    }
   ],
   "source": [
    "# fit the raw data into the vectorizer and tranform it into a series of arrays\n",
    "X_train_counts = cv.fit_transform(X_train)\n",
    "X_test_counts = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Naive Bayes classifier to train a new model and see if it works better. From the last section with out preprocessing or feature engineering, our precison, recall and F1 are in the mid 80s, but now we got 90 for each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.89      0.94      0.91       152\n",
      "           comp.graphics       0.72      0.90      0.80       188\n",
      " comp.os.ms-windows.misc       0.87      0.80      0.83       205\n",
      "comp.sys.ibm.pc.hardware       0.82      0.87      0.84       189\n",
      "   comp.sys.mac.hardware       0.98      0.82      0.89       199\n",
      "          comp.windows.x       0.89      0.90      0.89       202\n",
      "            misc.forsale       0.89      0.78      0.83       193\n",
      "               rec.autos       0.96      0.92      0.94       194\n",
      "         rec.motorcycles       0.97      0.95      0.96       180\n",
      "      rec.sport.baseball       0.97      0.92      0.95       209\n",
      "        rec.sport.hockey       0.83      0.98      0.90       186\n",
      "               sci.crypt       0.92      0.96      0.94       193\n",
      "         sci.electronics       0.96      0.86      0.90       208\n",
      "                 sci.med       0.96      0.94      0.95       202\n",
      "               sci.space       0.94      0.96      0.95       206\n",
      "  soc.religion.christian       0.90      0.95      0.93       211\n",
      "      talk.politics.guns       0.90      0.98      0.94       174\n",
      "   talk.politics.mideast       0.90      0.99      0.95       182\n",
      "      talk.politics.misc       0.95      0.92      0.93       164\n",
      "      talk.religion.misc       0.93      0.66      0.77       129\n",
      "\n",
      "             avg / total       0.91      0.90      0.90      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "predicted = clf.predict(X_test_counts)\n",
    "print(metrics.classification_report(y_test, predicted, labels=dataset.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember there are other vecotrizers that you can use? Walla, one of them is `TfidfVecotrizer`. LOL, what is tf-idf? It's on [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). It basically reflects how important a word/phrase is to a document in a corpus. The constructor of `TfidfVectorizer` takes in the same parameters as that of `CountVectorizer`, so you can perfrom the same preprocessing/feature extraction. Let's see if using tf-idf will help improve the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(preprocessor=normalize_numbers, ngram_range=(1,3), stop_words='english')\n",
    "X_train_tf = tv.fit_transform(X_train)\n",
    "X_test_tf = tv.transform(X_test)\n",
    "clf2 = MultinomialNB().fit(X_train_tf, y_train)\n",
    "predicted = clf2.predict(X_test_tf)\n",
    "print(metrics.classification_report(y_test, predicted, labels=dataset.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you like typing a longer blob of code, you can use the `TfidfTransformer` to transform a word count matrix created by `CountVectorizer` into a tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Selection\n",
    "\n",
    "Naive Bayes is the textbook model that people introduce in the first few NLP/ML classes, but it does not perform well when the size of data is relatively small. These days, Maximum Entropy is the go-to baseline classifier for many machine learning tasks. So let's try the [logistic regression classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in scikit-learn (Maxent and logistic regression are virtually the same thing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='sag', max_iter=100, n_jobs=4)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "predicted = clf.predict(X_test_counts)\n",
    "print(metrics.classification_report(y_test, predicted, labels=dataset.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
