{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn Introductory Tutorial\n",
    "\n",
    "scikit-learning is an open-sourced simple and efficient tools for data mining, data analysis and machine learning in Python. It is built on NumPy, SciPy and matplotlib. There are built-in classification, regression, and clustering models, as well as useful features like dimensionality reduction, evaluation and preprocessing. \n",
    "\n",
    "This tutorial is specifically tailored for NLP, i.e. working with text data. It will cover the following topics: loading data, preprocessing, feature extraction, training, evaluation, grid search, building a pipeline, creating custom transformers, etc.\n",
    "\n",
    "For this tutorial, we will use the 20 Newsgroups data set <http://qwone.com/~jason/20Newsgroups/> and perform topic classification. For the sake of time, I converted all the data into a CSV file. \n",
    "\n",
    "Note: apparently Jupyter disables spell-checker, so I'm only partially responsible for the typos in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset\n",
    "\n",
    "For the sake of convenience, we will use pandas to read CSV file. (You may do so with numpy as well; there is a `loadtext()` function, but you might encounter encoding issues when using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('20news-18828.csv', header=None, delimiter=',', names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 categories: True\n",
      "There are 18828 records: True\n"
     ]
    }
   ],
   "source": [
    "print(\"There are 20 categories: %s\" % (len(dataset.label.unique()) == 20))\n",
    "print(\"There are 18828 records: %s\" % (len(dataset) == 18828))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split it to train set and test set. To do so, we can use the `train_test_split()` function. In scikit-learn's convention, X indicates data (yeah, uppercase X), and y indicates truths (and yeah, lowercase y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.text, dataset.label, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A Simple Example\n",
    "\n",
    "Before going too much into preprocessing, feature extraction and other more complicated tasks, we will do a relatively simple but complete example. In this example, we will use bag-of-words as features, and Naive Bayes as classifier to establish our baseline.\n",
    "\n",
    "There are some built-in vectorizers, `CountVectorizer` and `TfidfVectorizer` that we can use to vectorizer our raw data and perform preprocessing and feature exctration on it. First, we will experiment with `CountVectorizer` which basically makes a token/ngram a feature and stores its count in the corresponding feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15062, 167363)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize a CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# fit the raw data into the vectorizer and tranform it into a series of arrays\n",
    "X_train_counts = cv.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar thing needs to be done for the test set, but we only need to use the `transform()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3766, 167363)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts = cv.transform(X_test)\n",
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit our features and labels into a Naive Bayes classifier, which basically trains a model. After training, we can use it to perform prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.ibm.pc.hardware comp.sys.ibm.pc.hardware\n",
      "alt.atheism alt.atheism\n",
      "comp.graphics comp.graphics\n",
      "rec.sport.hockey rec.sport.hockey\n",
      "sci.med sci.med\n",
      "alt.atheism alt.atheism\n",
      "talk.politics.guns talk.politics.guns\n",
      "rec.motorcycles rec.motorcycles\n",
      "rec.autos rec.autos\n",
      "rec.autos sci.electronics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "predicted = clf.predict(X_test_counts)\n",
    "\n",
    "# sample some of the predictions against the ground truths \n",
    "for prediction, truth in zip(predicted[:10], y_test[:10]):\n",
    "    print(prediction, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some legit evaluation. The `classification_report()` function gives you precison, recall and f1 scores for each label, and their average. If you want to calculate overall macro-averaged, micro-averaged or weighted performance, you can use the `precision_recall_fscore_support`. Finally, the `confusion_matrix()` can show you which labels are confusing to the model, but unfortunately, it does not include the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.91      0.88       171\n",
      "           comp.graphics       0.60      0.92      0.73       189\n",
      " comp.os.ms-windows.misc       0.98      0.21      0.34       191\n",
      "comp.sys.ibm.pc.hardware       0.64      0.82      0.72       194\n",
      "   comp.sys.mac.hardware       0.92      0.81      0.86       189\n",
      "          comp.windows.x       0.76      0.86      0.81       195\n",
      "            misc.forsale       0.94      0.68      0.79       195\n",
      "               rec.autos       0.89      0.94      0.91       191\n",
      "         rec.motorcycles       0.96      0.96      0.96       182\n",
      "      rec.sport.baseball       0.98      0.92      0.95       201\n",
      "        rec.sport.hockey       0.95      0.98      0.97       186\n",
      "               sci.crypt       0.86      0.93      0.89       210\n",
      "         sci.electronics       0.94      0.79      0.86       204\n",
      "                 sci.med       0.94      0.92      0.93       202\n",
      "               sci.space       0.93      0.94      0.94       199\n",
      "  soc.religion.christian       0.75      0.96      0.84       197\n",
      "      talk.politics.guns       0.93      0.95      0.94       196\n",
      "   talk.politics.mideast       0.88      0.97      0.93       184\n",
      "      talk.politics.misc       0.74      0.93      0.83       149\n",
      "      talk.religion.misc       0.97      0.49      0.65       141\n",
      "\n",
      "             avg / total       0.87      0.85      0.84      3766\n",
      "\n",
      "Micro-averaged Performance:\n",
      "Precision: 0.8489113117365905, Recall: 0.8489113117365905, F1: 0.8489113117365906\n",
      "[[155   1   0   0   0   0   0   0   1   0   0   0   0   0   0   6   0   6\n",
      "    0   2]\n",
      " [  0 174   0   3   0   5   0   1   0   0   0   1   0   2   0   1   0   0\n",
      "    2   0]\n",
      " [  0  45  40  50   4  38   1   0   0   0   0   7   3   0   0   0   0   0\n",
      "    3   0]\n",
      " [  1  11   0 160   5   6   2   2   0   0   0   1   2   1   1   0   0   0\n",
      "    2   0]\n",
      " [  0   8   0   9 153   2   1   0   0   0   0   6   2   1   2   1   0   2\n",
      "    2   0]\n",
      " [  0  18   1   2   0 167   0   0   2   0   0   1   0   2   1   0   0   1\n",
      "    0   0]\n",
      " [  0  11   0  15   3   0 133  15   1   3   0   3   1   0   1   2   0   2\n",
      "    5   0]\n",
      " [  0   0   0   0   0   0   1 179   2   0   0   1   2   0   0   0   1   0\n",
      "    5   0]\n",
      " [  0   0   0   0   0   0   2   1 174   0   0   0   0   1   0   1   1   1\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   2   1 185   8   0   0   1   1   0   0   0\n",
      "    3   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0 183   0   0   0   0   0   0   1\n",
      "    1   0]\n",
      " [  0   6   0   0   0   0   0   1   0   0   0 196   0   0   0   1   2   1\n",
      "    3   0]\n",
      " [  1   7   0  12   2   0   1   1   1   0   0  10 162   3   1   0   0   1\n",
      "    2   0]\n",
      " [  1   2   0   0   0   0   0   0   0   0   0   0   1 185   5   7   0   0\n",
      "    1   0]\n",
      " [  1   2   0   0   0   0   0   0   0   0   0   0   0   0 188   3   0   0\n",
      "    5   0]\n",
      " [  3   2   0   0   0   0   0   0   0   0   0   0   0   0   0 189   0   2\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   1 187   2\n",
      "    4   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   1   0   0   0   0   3   0 179\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   1   6   1\n",
      "  139   0]\n",
      " [ 18   0   0   0   0   0   0   0   0   0   0   0   0   1   0  37   4   4\n",
      "    8  69]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted, labels=dataset.label.unique()))\n",
    "\n",
    "p, r, f1, _ = metrics.precision_recall_fscore_support(y_test, predicted, labels=dataset.label.unique(), average='micro')\n",
    "\n",
    "print(\"Micro-averaged Performance:\\nPrecision: {0}, Recall: {1}, F1: {2}\".format(p, r, f1))\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, predicted, labels=dataset.label.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing & Feature Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15062, 167363)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
