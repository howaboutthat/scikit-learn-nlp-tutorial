{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn for NLP -- Part 2 Feature Engineering\n",
    "\n",
    "\n",
    "If you are reading this, that means you are still alive. Welcome back to the reality of learning scikit-learn. \n",
    "\n",
    "This tutorial focuses on feature engineering and covers more advanced topics in scikit-learn like feature extraction, building a pipeline, creating custom transformers, feature union, dimensionality reduction, etc. Feature engineering is a very important step in NLP and ML; it is not a trival task to select good features. Therefore, we are spending a lot of time on it here. \n",
    "\n",
    "Without further ado, let's start with loading a dataset again. This time, we will use a CSV file that has more than two columns, i.e. one column for labels and mutiple columns for raw data/features. This time, we are using a subset of the Yelp Review Data Challenge dataset. Just like the 20 News Group dataset, I converted this dataset to CSV. \n",
    "\n",
    "There are 5 star ratings (shocking), and I extracted 500 reviews for each rating. This dataset is small, because it's only intended for some quick demo; therefore, the performance of any classifier won't be too good. Other than extracing the text of each review, I also included other users' votes for each review, i.e funny, useful and cool. For this tutorial, our task is to predict the star rating of each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('yelp-review-subset.csv', header=0, delimiter=',', names=['stars', 'text', 'funny', 'useful', 'cool'])\n",
    "\n",
    "# just checking the dataset\n",
    "print('There are {0} star ratings, and {1} reviews'.format(len(dataset.stars.unique()), len(dataset)))\n",
    "print(dataset.stars.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pandas dataframe, it is very easy to select a subset of a dataframe by column names, simply pass in a list of column names. So we are going to split our data just like the previous tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[['text', 'funny', 'useful', 'cool']], dataset['stars'], train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference now is that we have four columns in our raw data. Three of them are ``funny``, ``useful`` and ``cool`` which contain numeric values, which is perfect for scikit-learn as it expects values of features to be numeric or indexes. What we need to do is to extract features from the ``text`` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first do something very similar to the previous tutorial, we initialize a ``CountVectorizer`` object and then pass raw text dat into its ``fit_transform()`` function to index the count of each word. Please note that you should not pass in the whole ``X_train`` dataframe into the function, but only the ``text`` column, i.e. the ``X_train.text`` dataframe (more or less like an array). Otherwise, it does not extract features from your ``text`` column, but simly indexes all the values in each column. Please see the shape of the output of two different dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize a CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# fit the raw data into the vectorizer and tranform it into a series of arrays\n",
    "X_train_counts = cv.fit_transform(X_train.text)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# this is not what you want.\n",
    "cv_test = CountVectorizer()\n",
    "X_train_counts_test = cv.fit_transform(X_train)\n",
    "print(X_train_counts_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a problem, the ``X_train_counts`` vector only contains features from ``text``, but now what should we do if we want to include ``funny``, ``useful`` and ``cool`` vote counts as feature as well? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  ``Pipeline`` and ``FeatureUnion`` \n",
    "\n",
    "To deal with that problem, we need to talk about [``Pipeline``](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [``FeatureUnion``](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html). ``Pipeline`` lets us define a list of steps which consists of a list of transformers to extract features from data (including ``FeatureUnion``) and a final estimator (aka classifier). ``FeatureUnion`` basically concatenates results of multiple transformer objects. The following is a complete example of how to use these two together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class ItemSelector(TransformerMixin):\n",
    "    \"\"\"This class allows you to select a subset of a dataframe based on given column name(s).\"\"\"\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe[self.keys]\n",
    "\n",
    "\n",
    "class VotesToDictTransformer(TransformerMixin):\n",
    "    \"\"\"This tranformer converts the vote counts of each row into a dictionary.\"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, votes):\n",
    "        funny, useful, cool = votes['funny'], votes['useful'], votes['cool']\n",
    "        return [{'funny': f, 'useful': u, 'cool': c} \n",
    "                for f, u, c in zip(funny, useful, cool)]\n",
    "    \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Use FeatureUnion to combine the features from text and votes\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for getting BOW features from the texts\n",
    "            ('bag-of-words', Pipeline([\n",
    "                ('selector', ItemSelector(keys='text')),\n",
    "                ('counts', CountVectorizer()),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for getting vote counts as features\n",
    "            # the DictVecotrizer object there transform indexes the values of the dictionaries\n",
    "            # passed down from the VotesToDictTransformer object.\n",
    "            ('votes', Pipeline([\n",
    "                ('selector', ItemSelector(keys=['funny', 'useful', 'cool'])),\n",
    "                ('votes_to_dict', VotesToDictTransformer()),\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'bag-of-words': 1.0,\n",
    "            'votes': 0.5\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a naive bayes classifier on the combined features\n",
    "    ('bayes', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predicted = pipeline.predict(X_test)\n",
    "print(classification_report(predicted, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom Transformers\n",
    "In the previous section, I defined two classes ``ItemSelector`` and ``VotesToDictTransformer``, and the commonality of these two is that they inherited the [``TransformerMixin``](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) class. ``TransformerMixin`` is the base class of many built-in transformers and vectorizers in scikit-learn, e.g. ``CountVectorizer``, ``TfidfVectorizer``, ``TfidfTransformer``, ``DictVectorizer``, etc. We define the ``transform()`` function to manipulate the data in a more custom way. For example, ``ItemSelector`` returns a subset of dataframe based on given column names, and ``VotesToDictTransformer`` transforms a dataframe into a list of dictionaries.  \n",
    "\n",
    "To demonstrate how useful customer transformers are, let's define another one. Say, we hypothesize that the sentiment of each review can be a strong feature for predict the star rating. Then we would need a ``SentimentTransformer`` class. \n",
    "\n",
    "To avoid spending time to train our own sentiment classifier, we use the ``TextBlob`` package for its built-in [sentiment analysis feature](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "class SentimentTransformer(TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            blob = TextBlob(text)\n",
    "            features.append({'polarity': binarize_number(blob.sentiment.polarity),\n",
    "                             'subjectivity': binarize_number(blob.sentiment.subjectivity)})\n",
    "        return features\n",
    "\n",
    "\n",
    "def binarize_number(num):\n",
    "    return 0 if num < 0.5 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add that transformer to our existing pipeline, and see if the additional features help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        \n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                    \n",
    "            ('bag-of-words', Pipeline([\n",
    "                ('selector', ItemSelector(keys='text')),\n",
    "                ('counts', CountVectorizer()),\n",
    "            ])),\n",
    "\n",
    "            ('votes', Pipeline([\n",
    "                ('selector', ItemSelector(keys=['funny', 'useful', 'cool'])),\n",
    "                ('votes_to_dict', VotesToDictTransformer()),\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "                    \n",
    "            ('sentiments', Pipeline([\n",
    "                ('selector', ItemSelector(keys='text')),\n",
    "                ('sentiment_transform', SentimentTransformer()),\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'bag-of-words': 1.0,\n",
    "            'votes': 0.5,\n",
    "            'sentiments': 1.0,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a naive bayes classifier on the combined features\n",
    "    ('bayes', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predicted = pipeline.predict(X_test)\n",
    "print(classification_report(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
